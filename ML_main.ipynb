{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8dfc9aa-f935-4b71-ba4d-981408242f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl, pandas as pd, numpy as np, json\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import lightgbm as lgb, xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === 全域參數設定 ===\n",
    "n_clusters = 5\n",
    "top_k_features = 36\n",
    "pca_components = 10\n",
    "n_selected_features = 400\n",
    "pseudo_rounds = 10                     # 最大 pseudo-labeling 次數\n",
    "max_total_pos = 20000                  # 最多可以加多少個 pseudo-positive\n",
    "max_pos_per_round = 1500              # 每一輪最多加幾個 pseudo-positive\n",
    "total_pos_added = 0                  # 用來追蹤目前已加了幾個 pseudo-positive\n",
    "neg_sample_ratio = 1                 # 每個 positive 對應的負樣本比例\n",
    "base_clip = 0.1                      # 錯誤 pseudo-label 的 clip 下限\n",
    "max_clip = 1                         # 正確 pseudo-label 的權重上限\n",
    "n_splits_p = 5                       # pseudo-label 用的內部 CV 次數\n",
    "top_pct = 0.00731838457              # private set top x% 要預測為正樣本\n",
    "fixed_test_topk = 176               # public test set 固定取前 k 個當作正樣本\n",
    "top_k = 200\n",
    "\n",
    "# === 評估函式定義 ===\n",
    "def add_features(X, cluster_labels=None, top_k_features=10):\n",
    "    df = X.copy()\n",
    "    df[\"row_mean\"] = df.mean(axis=1)\n",
    "    df[\"row_max\"] = df.max(axis=1)\n",
    "    df[\"row_min\"] = df.min(axis=1)\n",
    "    df[\"row_std\"] = df.std(axis=1)\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    df[\"row_skew\"] = df.apply(skew, axis=1)\n",
    "    df[\"row_kurt\"] = df.apply(kurtosis, axis=1)\n",
    "\n",
    "    cols = df.columns[:top_k_features]\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            df[f\"{cols[i]}_x_{cols[j]}\"] = df[cols[i]] * df[cols[j]]\n",
    "\n",
    "    scaled_data = StandardScaler().fit_transform(df.fillna(0))\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca_out = pca.fit_transform(scaled_data)\n",
    "    for i in range(pca_components):\n",
    "        df[f\"pca_{i+1}\"] = pca_out[:, i]\n",
    "\n",
    "    if cluster_labels is not None:\n",
    "        df[\"cluster\"] = cluster_labels\n",
    "        cluster_mean = df.groupby(\"cluster\").transform(\"mean\")\n",
    "        original_cols = [col for col in df.columns if col != \"cluster\"]\n",
    "        diff_df = df[original_cols] - cluster_mean[original_cols]\n",
    "        diff_df = diff_df.add_suffix(\"_diff_cluster_mean\")\n",
    "        df = pd.concat([df.drop(columns=\"cluster\"), diff_df], axis=1)\n",
    "    return df\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred_label = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_label), True\n",
    "def fit_lgbm_with_early_stopping(model, X, y, sample_weight=None, eval_ratio=0.3, random_state=42):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=eval_ratio, stratify=y, random_state=random_state)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = np.array(sample_weight)\n",
    "        sw_tr, sw_val = train_test_split(sample_weight, test_size=eval_ratio, stratify=y, random_state=random_state)\n",
    "    else:\n",
    "        sw_tr = sw_val = None\n",
    "    model.fit(X_tr, y_tr,sample_weight=sw_tr, eval_set=[(X_val, y_val)], eval_metric=\"binary_logloss\",\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(period=0)])\n",
    "    return model\n",
    "def fit_xgb_with_early_stopping(model, X, y, sample_weight=None, eval_ratio=0.3, random_state=42):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=eval_ratio, stratify=y, random_state=random_state)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = np.array(sample_weight)\n",
    "        sw_tr, sw_val = train_test_split(sample_weight, test_size=eval_ratio, stratify=y, random_state=random_state)\n",
    "    else:\n",
    "        sw_tr = sw_val = None\n",
    "    model.fit(X_tr, y_tr, sample_weight=sw_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return model\n",
    "def fit_cat_with_early_stopping(model, X, y, sample_weight=None, eval_ratio=0.3, random_state=42):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=eval_ratio, stratify=y, random_state=random_state)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = np.array(sample_weight)\n",
    "        sw_tr, sw_val = train_test_split(sample_weight, test_size=eval_ratio, stratify=y, random_state=random_state)\n",
    "    else:\n",
    "        sw_tr = sw_val = None\n",
    "    model.fit(X_tr, y_tr,sample_weight=sw_tr,eval_set=(X_val, y_val),early_stopping_rounds=50,verbose=0)\n",
    "    return model\n",
    "def sample_negatives(n, seed=42):\n",
    "    n=int(n)\n",
    "    available_indices = list(neg_pool_indices)\n",
    "    selected_idx = np.random.choice(available_indices, size=n, replace=True)\n",
    "    neg_pool_indices.difference_update(selected_idx)\n",
    "    return selected_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124d82f-96cd-4606-bf90-6e54aaa7e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 讀取經過特徵工程的 parquet 檔 ===\n",
    "df_train_fe = pd.read_parquet(\"FILE_PATH\")\n",
    "df_test_fe = pd.read_parquet(\"FILE_PATH\")\n",
    "\n",
    "# === 拆分欄位 ===\n",
    "X_train = df_train_fe.drop(columns=[\"ID\", \"飆股\"])\n",
    "y_train = df_train_fe[\"飆股\"].to_numpy()\n",
    "X_test = df_test_fe.drop(columns=[\"ID\", \"dataset\"])\n",
    "ids = df_test_fe[\"ID\"].to_numpy()\n",
    "dataset_flag = df_test_fe[\"dataset\"].to_numpy\n",
    "is_test = (dataset_flag == \"test\")\n",
    "is_private = (dataset_flag == \"private\")\n",
    "\n",
    "# === 載入已選好的特徵名稱，僅取前top_k ===\n",
    "with open(\"FILE_PATH\", \"r\") as f:\n",
    "    selected_feature_names = json.load(f)\n",
    "selected_feature_names = selected_feature_names[:top_k]\n",
    "\n",
    "# === 套用前200個特徵 ===\n",
    "X_train = X_train[selected_feature_names]\n",
    "X_test = X_test[selected_feature_names]\n",
    "# === 載入模型參數 ===\n",
    "with open(\"CAT_params.json\", \"r\") as f: params_cat = json.load(f)\n",
    "with open(\"LGB_params.json\", \"r\") as f: params_lgb = json.load(f)\n",
    "with open(\"XGB_params.json\", \"r\") as f: params_xgb = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d0023-dfa3-4d1f-b32a-0060abbca892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 特徵工程 + 特徵選擇：只有在 JSON 不存在時才執行 ===\n",
    "if os.path.exists(\"FILE_PATH\"):\n",
    "    print(\"已偵測到 features，跳過特徵選擇\")\n",
    "\n",
    "    with open(\"FILE_PATH\", \"r\") as f:\n",
    "        selected_feature_names = json.load(f)\n",
    "\n",
    "    X_all = pd.concat([X_train_raw, X_test_raw]).fillna(0)\n",
    "    X_scaled = StandardScaler().fit_transform(X_all)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_scaled)\n",
    "    \n",
    "    X_train = add_features(X_train_raw, cluster_labels=kmeans.labels_[:len(X_train_raw)], top_k_features=top_k_features)\n",
    "    X_test = add_features(X_test_raw, cluster_labels=kmeans.labels_[len(X_train_raw):], top_k_features=top_k_features)\n",
    "\n",
    "    X_train = X_train[selected_feature_names]\n",
    "    X_test = X_test[selected_feature_names]\n",
    "\n",
    "else:\n",
    "    print(\" 執行特徵工程與 LGB 特徵選擇...\")\n",
    "    # 合併 train+test 做聚類\n",
    "    X_all = pd.concat([X_train_raw, X_test_raw]).fillna(0)\n",
    "    X_scaled = StandardScaler().fit_transform(X_all)\n",
    "\n",
    "    # 聚類\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_scaled)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # 特徵擴充（交叉 + PCA + 群差）\n",
    "    X_train = add_features(X_train_raw, cluster_labels=labels[:len(X_train_raw)], top_k_features=top_k_features)\n",
    "    X_test = add_features(X_test_raw, cluster_labels=labels[len(X_train_raw):], top_k_features=top_k_features)\n",
    "\n",
    "    # 特徵選擇（用 LGB）\n",
    "    lgb_fs = lgb.LGBMClassifier(**params_lgb)\n",
    "    lgb_fs.fit(X_train, y_train)\n",
    "    importances = lgb_fs.feature_importances_\n",
    "    top_k_idx = np.argsort(importances)[::-1][:n_selected_features]\n",
    "    selected_feature_names = X_train.columns[top_k_idx].tolist()\n",
    "\n",
    "    # 儲存下來\n",
    "    with open(\"selected_features.json\", \"w\") as f:\n",
    "        json.dump(selected_feature_names, f)\n",
    "\n",
    "    # 保留特徵\n",
    "    X_train = X_train[selected_feature_names]\n",
    "    X_test = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1dffc-75d7-4e9c-a946-3a2dc2bcaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 分割訓練集做初始驗證 ===\n",
    "X_train_all = X_train.copy()\n",
    "y_train_all = y_train.copy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.3, stratify=y_train_all, random_state=42)\n",
    "\n",
    "# === 計算 sample weight（用於 LGB / XGB） ===\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "full_sample_weight = np.where(y_train == 1, scale_pos_weight, 1.0)\n",
    "\n",
    "# === 初始模型交叉驗證訓練 ===\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "auc_list, f1_list = [], []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    train_weight = full_sample_weight[train_idx]\n",
    "    \n",
    "    model_lgb = fit_lgbm_with_early_stopping(lgb.LGBMClassifier(**params_lgb), X_tr, y_tr, sample_weight=train_weight)\n",
    "    model_xgb = fit_xgb_with_early_stopping(xgb.XGBClassifier(**params_xgb), X_tr, y_tr, sample_weight=train_weight)\n",
    "    model_cat = fit_cat_with_early_stopping(CatBoostClassifier(**params_cat), X_tr, y_tr, sample_weight=train_weight)\n",
    "\n",
    "    probs_lgb = model_lgb.predict_proba(X_val)[:, 1]\n",
    "    probs_xgb = model_xgb.predict_proba(X_val)[:, 1]\n",
    "    probs_cat = model_cat.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    rank_lgb = probs_lgb.argsort().argsort()\n",
    "    rank_xgb = probs_xgb.argsort().argsort()\n",
    "    rank_cat = probs_cat.argsort().argsort()\n",
    "    ensemble_rank = 0.6 * rank_lgb + 0.2 * rank_xgb + 0.2 * rank_cat\n",
    "\n",
    "    n_top = max(1, int(len(y_val) * 0.00731838457))\n",
    "    pred_ensemble = np.zeros_like(y_val)\n",
    "    pred_ensemble[np.argsort(-ensemble_rank)[:n_top]] = 1\n",
    "\n",
    "    auc_ens = roc_auc_score(y_val, (ensemble_rank - ensemble_rank.min()) / (ensemble_rank.max() - ensemble_rank.min()))\n",
    "    f1_ens = f1_score(y_val, pred_ensemble)\n",
    "    auc_list.append(auc_ens)\n",
    "    f1_list.append(f1_ens)\n",
    "\n",
    "    print(f\"[Fold {fold+1}] ENSEMBLE | AUC = {auc_ens:.4f} | F1 = {f1_ens:.4f}\")\n",
    "\n",
    "print(f\"\\n[SK-Fold Validation Result (With Feature Selection)]\")\n",
    "print(f\"Average AUC = {np.mean(auc_list):.4f} | Average F1 = {np.mean(f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0152c-bb49-410e-b41e-68b33da4b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 建立正負樣本池 ===\n",
    "X_pos_real = X_train[y_train == 1].reset_index(drop=True)\n",
    "y_pos_real = np.ones(len(X_pos_real))\n",
    "X_neg_all = X_train[y_train == 0].reset_index(drop=True)\n",
    "y_neg_all = y_train[y_train == 0].copy()\n",
    "neg_pool_indices = set(range(len(X_neg_all)))\n",
    "#neg_sample_idx = sample_negatives(len(X_pos_real) * neg_sample_ratio)\n",
    "neg_sample_idx = sample_negatives(int(len(X_neg_all)*0.8))\n",
    "X_neg_real = X_neg_all.iloc[neg_sample_idx].reset_index(drop=True)\n",
    "y_neg_real = y_neg_all[neg_sample_idx]\n",
    "X_aug = pd.concat([X_pos_real, X_neg_real], ignore_index=True)\n",
    "y_aug = np.concatenate([y_pos_real, y_neg_real])\n",
    "sample_weight = np.ones(len(y_aug))\n",
    "last_f1, no_improve_rounds = 0, 0\n",
    "mask_pool = np.ones(len(X_train), dtype=bool)\n",
    "all_error_features, all_is_correct = [], []\n",
    "error_detector = None\n",
    "pseudo_pos_counter = defaultdict(int)\n",
    "\n",
    "# === Pseudo-labeling 主迴圈 ===\n",
    "for pseudo_round in range(pseudo_rounds):\n",
    "    # (1) 模型訓練\n",
    "    model_lgb = fit_lgbm_with_early_stopping(model_lgb, X_aug, y_aug, sample_weight)\n",
    "    model_xgb = fit_xgb_with_early_stopping(model_xgb, X_aug, y_aug, sample_weight)\n",
    "    model_cat = fit_cat_with_early_stopping(model_cat, X_aug, y_aug, sample_weight)\n",
    "    \n",
    "    # 預測 pseudo pool\n",
    "    X_pool = X_train[mask_pool]\n",
    "    y_pool = y_train[mask_pool]\n",
    "    n_candidates = len(X_pool)\n",
    "    \n",
    "    probs_lgb = model_lgb.predict_proba(X_pool)[:, 1]\n",
    "    probs_xgb = model_xgb.predict_proba(X_pool)[:, 1]\n",
    "    probs_cat = model_cat.predict_proba(X_pool)[:, 1]\n",
    "    \n",
    "    # Rank-based Ensemble\n",
    "    rank_lgb = probs_lgb.argsort().argsort()\n",
    "    rank_xgb = probs_xgb.argsort().argsort()\n",
    "    rank_cat = probs_cat.argsort().argsort()\n",
    "    ensemble_rank = 0.6 * rank_lgb + 0.2 * rank_xgb + 0.2 * rank_cat\n",
    "    ensemble_probs = ensemble_rank  # 用於篩選 top-k\n",
    "    \n",
    "    # 一致性條件\n",
    "    cm = ((np.abs(probs_lgb - probs_xgb) < 0.05) & (np.abs(probs_cat - probs_lgb) < 0.05))\n",
    "    \n",
    "    # (2) 找出可信度高的 pseudo-positive (Top-K + 一致性過濾)\n",
    "    n_top_pseudo = min(max_pos_per_round, max_total_pos - total_pos_added)\n",
    "    top_k_indices = np.argsort(ensemble_probs)[:n_top_pseudo]\n",
    "    pool_indices = np.where(mask_pool)[0]\n",
    "    pos_indices_global = pool_indices[top_k_indices]\n",
    "    valid_pos = [i for i, gidx in enumerate(pos_indices_global) if pseudo_pos_counter[gidx] < 10]\n",
    "    \n",
    "    top_k_indices = top_k_indices[valid_pos]\n",
    "    pos_idx = top_k_indices[cm[top_k_indices]]\n",
    "    \n",
    "    # 避免超過 max_total_pos\n",
    "    remaining_pos = max_total_pos - total_pos_added\n",
    "    available_this_round = min(remaining_pos, max_pos_per_round)\n",
    "    if len(pos_idx) > available_this_round:\n",
    "        pos_idx = pos_idx[np.argsort(ensemble_probs[pos_idx])[:available_this_round]]\n",
    "    \n",
    "    total_pos_added += len(pos_idx)\n",
    "    \n",
    "    # (3) 組成新一輪 pseudo 樣本\n",
    "    neg_sample_idx = sample_negatives(len(pos_idx) * neg_sample_ratio)\n",
    "    X_new_neg = X_neg_all.iloc[neg_sample_idx].reset_index(drop=True)\n",
    "    y_new_neg = y_neg_all[neg_sample_idx]\n",
    "\n",
    "    #neg_sample_idx = np.random.choice(len(X_neg_all), size=int(0.8 * len(X_neg_all)), replace=False)\n",
    "    #X_new_neg = X_neg_all.iloc[neg_sample_idx].reset_index(drop=True)\n",
    "    #y_new_neg = y_neg_all[neg_sample_idx]\n",
    "    #X_new_neg = X_neg_all.reset_index(drop=True)\n",
    "    #y_new_neg = y_neg_all\n",
    "\n",
    "    X_pseudo = pd.concat([X_pool.iloc[pos_idx], X_new_neg], ignore_index=True)\n",
    "    y_pseudo_raw = np.concatenate([np.ones(len(pos_idx)), np.zeros(len(X_new_neg))])\n",
    "    true_labels = np.concatenate([y_pool[pos_idx], y_new_neg])\n",
    "    \n",
    "    # 使用 rank 越小表示信心越高 => confidence = 1 - (rank / max_rank)\n",
    "    pseudo_conf = np.concatenate([\n",
    "        1 - (ensemble_rank[pos_idx] / (np.max(ensemble_rank) + 1e-6)),\n",
    "        np.zeros(len(X_new_neg))  # negative 的 confidence 是 0\n",
    "    ])\n",
    "    \n",
    "    # 紀錄被使用的 pseudo 樣本索引\n",
    "    pool_indices = np.where(mask_pool)[0]\n",
    "    pos_indices_global = pool_indices[pos_idx]\n",
    "    for gidx in pos_indices_global:\n",
    "        pseudo_pos_counter[gidx] += 1\n",
    "    n_neg = len(X_new_neg)\n",
    "    zero_logits = np.zeros(n_neg)\n",
    "    \n",
    "    # (4) 建構錯誤偵測器（從信心不一緻度估計）\n",
    "    ef = np.column_stack([\n",
    "        np.concatenate([probs_lgb[pos_idx], zero_logits]),\n",
    "        np.concatenate([probs_xgb[pos_idx], zero_logits]),\n",
    "        np.concatenate([probs_cat[pos_idx], zero_logits]),pseudo_conf,\n",
    "        np.abs(np.concatenate([probs_lgb[pos_idx], zero_logits]) - np.concatenate([probs_cat[pos_idx], zero_logits])),\n",
    "        np.std([\n",
    "            np.concatenate([probs_lgb[pos_idx], zero_logits]),\n",
    "            np.concatenate([probs_xgb[pos_idx], zero_logits]),\n",
    "            np.concatenate([probs_cat[pos_idx], zero_logits])], axis=0)])\n",
    "    all_error_features.append(ef)\n",
    "    all_is_correct.append((y_pseudo_raw == true_labels).astype(int))\n",
    "    if pseudo_round >= 2:\n",
    "        all_err_X = np.vstack(all_error_features)\n",
    "        all_err_y = np.concatenate(all_is_correct)\n",
    "        if len(np.unique(all_err_y)) < 2:\n",
    "            print(f\"[Pseudo Round {pseudo_round}] Error detector 未訓練（只有一類）\")\n",
    "            error_detector = None\n",
    "        else:\n",
    "            error_detector = lgb.LGBMClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.05,\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            X_tr_ed, X_val_ed, y_tr_ed, y_val_ed = train_test_split(\n",
    "                all_err_X, all_err_y, test_size=0.2, stratify=all_err_y, random_state=42\n",
    "            )\n",
    "            error_detector.fit(\n",
    "                X_tr_ed, y_tr_ed,\n",
    "                eval_set=[(X_val_ed, y_val_ed)],\n",
    "                eval_metric='auc',\n",
    "                callbacks=[lgb.early_stopping(20), lgb.log_evaluation(period=0)]\n",
    "            )\n",
    "            train_pred = error_detector.predict(X_val_ed)\n",
    "            train_acc = np.mean(train_pred == y_val_ed)\n",
    "            print(f\"[Pseudo Round {pseudo_round}] Error Detector (LGB) Valid Accuracy = {train_acc:.4f}\")\n",
    "            if train_acc < 0.6:\n",
    "                print(f\"[Pseudo Round {pseudo_round}] Error Detector 表現太差（< 0.6），跳過使用\")\n",
    "                error_detector = None\n",
    "    if error_detector:\n",
    "        proba = error_detector.predict_proba(ef)\n",
    "        if proba.shape[1] == 2:\n",
    "            error_pred = proba[:, 1]\n",
    "        else:\n",
    "            error_pred = np.ones(len(ef))\n",
    "        keep_mask = error_pred < 0.5\n",
    "        kept = keep_mask.sum()\n",
    "        if kept < 10:\n",
    "            print(f\"[Pseudo Round {pseudo_round}] Too few pseudo kept ({kept}/{len(keep_mask)}), fallback to original\")\n",
    "            keep_mask[:] = True\n",
    "        else:\n",
    "            print(f\"[Pseudo Round {pseudo_round}] Kept {kept}/{len(keep_mask)} pseudo samples after filtering\")\n",
    "        X_pseudo = X_pseudo[keep_mask]\n",
    "        y_pseudo_raw = y_pseudo_raw[keep_mask]\n",
    "        true_labels = true_labels[keep_mask]\n",
    "        pseudo_conf = pseudo_conf[keep_mask]\n",
    "    \n",
    "    # (5) 過濾錯誤 pseudo 並加入新的訓練樣本\n",
    "    skf_check = StratifiedKFold(n_splits=n_splits_p, shuffle=True, random_state=42)\n",
    "    cv_probs = np.zeros(len(y_aug))\n",
    "    for tr, val in skf_check.split(X_aug, y_aug):\n",
    "        X_tr, X_val = X_aug.iloc[tr], X_aug.iloc[val]\n",
    "        y_tr, y_val = y_aug[tr], y_aug[val]\n",
    "        w_tr = sample_weight[tr]\n",
    "        l = lgb.LGBMClassifier(**model_lgb.get_params())\n",
    "        l = fit_lgbm_with_early_stopping(l, X_tr, y_tr, sample_weight=w_tr)\n",
    "        x = xgb.XGBClassifier(**model_xgb.get_params())\n",
    "        x = fit_xgb_with_early_stopping(x, X_tr, y_tr, sample_weight=w_tr)\n",
    "        c = CatBoostClassifier(**model_cat.get_params())\n",
    "        c = fit_cat_with_early_stopping(c, X_tr, y_tr, sample_weight=w_tr)\n",
    "        val_l, val_x, val_c = l.predict_proba(X_val)[:, 1], x.predict_proba(X_val)[:, 1], c.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        s = np.vstack([val_l, val_x, val_c])\n",
    "        sm = np.exp(s) / np.sum(np.exp(s), axis=0)\n",
    "        cv_probs[val] = np.sum(s * sm, axis=0)\n",
    "        rank_l = val_l.argsort().argsort()\n",
    "        rank_x = val_x.argsort().argsort()\n",
    "        rank_c = val_c.argsort().argsort()\n",
    "        cv_probs[val] = (rank_l + rank_x + rank_c) / 3  # rank average\n",
    "\n",
    "    n_top = max(1, int(np.sum(y_aug))) \n",
    "    threshold = np.sort(cv_probs)[-n_top]\n",
    "    curr_f1 = f1_score(y_aug, (cv_probs >= threshold).astype(int))\n",
    "    n_incorrect = np.sum(y_pseudo_raw != true_labels)\n",
    "    if len(y_pseudo_raw) > 0:\n",
    "        n_incorrect = np.sum(y_pseudo_raw != true_labels)\n",
    "        pseudo_precision = 1 - n_incorrect / len(y_pseudo_raw)\n",
    "        pseudo_error_rate = n_incorrect / len(y_pseudo_raw)\n",
    "        clip_min = base_clip + (max_clip - base_clip) * pseudo_error_rate\n",
    "    else:\n",
    "        n_incorrect = 0\n",
    "        pseudo_precision = np.nan\n",
    "        clip_min = max_clip\n",
    "    pseudo_weights = [conf if p == t else clip_min for p, t, conf in zip(y_pseudo_raw, true_labels, pseudo_conf)]\n",
    "    X_aug = pd.concat([X_aug, X_pseudo], ignore_index=True)\n",
    "    y_aug = np.concatenate([y_aug, true_labels])\n",
    "    sample_weight = np.concatenate([sample_weight, pseudo_weights])\n",
    "    print(f\"[Pseudo Round {pseudo_round}] F1 = {curr_f1:.4f} / 上一輪 = {last_f1:.4f} | P: {len(pos_idx)}, N: {len(X_new_neg)}, Total: {len(y_aug)}\")\n",
    "    print(f\"[Pseudo Round {pseudo_round}] 錯誤樣本: {n_incorrect} / {len(y_pseudo_raw)}（Precision = {pseudo_precision:.4f}）\" \n",
    "          if len(y_pseudo_raw) > 0 else f\"[Pseudo Round {pseudo_round}] 沒有加入 pseudo 樣本\")\n",
    "    if error_detector and 'keep_mask' in locals():\n",
    "        print(f\"[Pseudo Round {pseudo_round}] keep_mask true count: {keep_mask.sum()} / {len(keep_mask)}\")\n",
    "    else:\n",
    "        print(f\"[Pseudo Round {pseudo_round}] 沒有使用 error detector，跳過 keep_mask\")\n",
    "        \n",
    "    # (6) 檢查是否 early stop\n",
    "    if curr_f1 - last_f1 < 0.005:\n",
    "        no_improve_rounds += 1\n",
    "        if no_improve_rounds >= 2: break\n",
    "    else:\n",
    "        no_improve_rounds = 0\n",
    "    last_f1 = curr_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dbd2a-0e96-4d3b-a84d-aac06b32954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 最終 5-Fold Ensemble 模型訓練與驗證 ===\n",
    "print(\"\\n[SK-Fold] 開始對 pseudo-label 擴充後的資料做交叉驗證與 Meta-Features 建構...\")\n",
    "final_auc_list, final_f1_list = [], []\n",
    "model_lgbs, model_xgbs, model_cats = [], [], []\n",
    "\n",
    "meta_X = np.zeros((len(X_aug), 16))\n",
    "meta_test = np.zeros((len(X_test), 16))\n",
    "\n",
    "probs_lgb_test_all = []\n",
    "probs_xgb_test_all = []\n",
    "probs_cat_test_all = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_aug, y_aug)):\n",
    "    X_tr, X_val = X_aug.iloc[tr_idx], X_aug.iloc[val_idx]\n",
    "    y_tr, y_val = y_aug[tr_idx], y_aug[val_idx]\n",
    "    w_tr = sample_weight[tr_idx]\n",
    "\n",
    "\n",
    "    model_lgb_f = fit_lgbm_with_early_stopping(lgb.LGBMClassifier(**model_lgb.get_params()), X_tr, y_tr, sample_weight=w_tr)\n",
    "    model_xgb_f = fit_xgb_with_early_stopping(xgb.XGBClassifier(**model_xgb.get_params()), X_tr, y_tr, sample_weight=w_tr)\n",
    "    model_cat_f = fit_cat_with_early_stopping(CatBoostClassifier(**model_cat.get_params()), X_tr, y_tr, sample_weight=w_tr)\n",
    "\n",
    "    # 儲存模型（後面 ensemble 用）\n",
    "    model_lgbs.append(model_lgb_f)\n",
    "    model_xgbs.append(model_xgb_f)\n",
    "    model_cats.append(model_cat_f)\n",
    "\n",
    "    # Validation 預測\n",
    "    val_lgb = model_lgb_f.predict_proba(X_val)[:, 1]\n",
    "    val_xgb = model_xgb_f.predict_proba(X_val)[:, 1]\n",
    "    val_cat = model_cat_f.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Test 預測\n",
    "    test_lgb = model_lgb_f.predict_proba(X_test)[:, 1]\n",
    "    test_xgb = model_xgb_f.predict_proba(X_test)[:, 1]\n",
    "    test_cat = model_cat_f.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # === Rank Ensemble 預測與評估 ===\n",
    "    rank_lgb = val_lgb.argsort().argsort()\n",
    "    rank_xgb = val_xgb.argsort().argsort()\n",
    "    rank_cat = val_cat.argsort().argsort()\n",
    "    ensemble_rank = 0.6 * rank_lgb + 0.2 * rank_xgb + 0.2 * rank_cat\n",
    "    val_probs = ensemble_rank\n",
    "    auc = roc_auc_score(y_val, (val_probs - val_probs.min()) / (val_probs.max() - val_probs.min()))\n",
    "    n_top = max(1, int(len(y_val) * top_pct))\n",
    "    f1 = f1_score(y_val, (val_probs >= np.sort(val_probs)[-n_top]).astype(int))\n",
    "    final_auc_list.append(auc)\n",
    "    final_f1_list.append(f1)\n",
    "    print(f\"[Fold {fold+1}] AUC = {auc:.4f} | F1 = {f1:.4f}\")\n",
    "\n",
    "    # === 蒐集 Meta-Features ===\n",
    "    s_val = np.vstack([val_lgb, val_xgb, val_cat])\n",
    "    s_test = np.vstack([test_lgb, test_xgb, test_cat])\n",
    "    r_val = np.vstack([val_lgb.argsort().argsort()/len(val_lgb), val_xgb.argsort().argsort()/len(val_xgb), val_cat.argsort().argsort()/len(val_cat)])\n",
    "    r_test = np.vstack([test_lgb.argsort().argsort()/len(test_lgb), test_xgb.argsort().argsort()/len(test_xgb), test_cat.argsort().argsort()/len(test_cat)])\n",
    "    sm_val = np.exp(s_val) / np.sum(np.exp(s_val), axis=0)\n",
    "    sm_test = np.exp(s_test) / np.sum(np.exp(s_test), axis=0)\n",
    "    top1_same = ((r_val[0].argmin() == r_val[1].argmin()) & (r_val[1].argmin() == r_val[2].argmin())).astype(float)\n",
    "    top1_same_test = ((r_test[0].argmin() == r_test[1].argmin()) & (r_test[1].argmin() == r_test[2].argmin())).astype(float)\n",
    "    rank_diff = np.ptp(r_val, axis=0)\n",
    "    rank_diff_test = np.ptp(r_test, axis=0)\n",
    "    rank_bias = np.abs(r_val - np.mean(r_val, axis=0, keepdims=True)).mean(axis=0)\n",
    "    rank_bias_test = np.abs(r_test - np.mean(r_test, axis=0, keepdims=True)).mean(axis=0)\n",
    "\n",
    "    # 填入 meta_X\n",
    "    meta_X[val_idx, :3] = s_val.T\n",
    "    meta_X[val_idx, 3] = np.min(s_val, axis=0)\n",
    "    meta_X[val_idx, 4] = np.max(s_val, axis=0)\n",
    "    meta_X[val_idx, 5] = np.mean(s_val, axis=0)\n",
    "    meta_X[val_idx, 6] = np.std(s_val, axis=0)\n",
    "    meta_X[val_idx, 7:10] = r_val.T\n",
    "    meta_X[val_idx, 10:12] = np.stack([np.mean(r_val, axis=0), np.std(r_val, axis=0)], axis=1)\n",
    "    meta_X[val_idx, 12] = np.mean(sm_val, axis=0)\n",
    "    meta_X[val_idx, 13] = top1_same\n",
    "    meta_X[val_idx, 14] = rank_diff\n",
    "    meta_X[val_idx, 15] = rank_bias\n",
    "\n",
    "    # 累積 meta_test（平均每折結果）\n",
    "    meta_test[:, :3] += s_test.T / skf.n_splits\n",
    "    meta_test[:, 3] += np.min(s_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 4] += np.max(s_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 5] += np.mean(s_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 6] += np.std(s_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 7:10] += r_test.T / skf.n_splits\n",
    "    meta_test[:, 10] += np.mean(r_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 11] += np.std(r_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 12] += np.mean(sm_test, axis=0) / skf.n_splits\n",
    "    meta_test[:, 13] += top1_same_test / skf.n_splits\n",
    "    meta_test[:, 14] += rank_diff_test / skf.n_splits\n",
    "    meta_test[:, 15] += rank_bias_test / skf.n_splits\n",
    "\n",
    "print(f\"\\n [Pseudo-After SK-Fold] 平均 AUC = {np.mean(final_auc_list):.4f} | 平均 F1 = {np.mean(final_f1_list):.4f}\")\n",
    "\n",
    "# === 結果後處理：test 固定取前 k 個，private 取前 p% ===\n",
    "probs_lgb_test = np.mean([m.predict_proba(X_test)[:, 1] for m in model_lgbs], axis=0)\n",
    "probs_xgb_test = np.mean([m.predict_proba(X_test)[:, 1] for m in model_xgbs], axis=0)\n",
    "probs_cat_test = np.mean([m.predict_proba(X_test)[:, 1] for m in model_cats], axis=0)\n",
    "rank_lgb = probs_lgb_test.argsort().argsort()\n",
    "rank_xgb = probs_xgb_test.argsort().argsort()\n",
    "rank_cat = probs_cat_test.argsort().argsort()\n",
    "ensemble_rank = 0.6 * rank_lgb + 0.2 * rank_xgb + 0.2 * rank_cat\n",
    "\n",
    "final_preds_pseudo_ensemble = np.zeros_like(ensemble_rank , dtype=int)\n",
    "test_probs = ensemble_rank[is_test]\n",
    "top_test_idx = np.argsort(-test_probs)[:fixed_test_topk]\n",
    "final_preds_pseudo_ensemble[is_test] = 0\n",
    "final_preds_pseudo_ensemble[np.where(is_test)[0][top_test_idx]] = 1\n",
    "private_probs = ensemble_rank[is_private]\n",
    "n_top_private = max(1, int(len(private_probs) * top_pct))\n",
    "top_private_idx = np.argsort(-private_probs)[:n_top_private]\n",
    "final_preds_pseudo_ensemble[is_private] = 0\n",
    "final_preds_pseudo_ensemble[np.where(is_private)[0][top_private_idx]] = 1\n",
    "\n",
    "pd.DataFrame({\"ID\": ids, \"飆股\": final_preds_pseudo_ensemble}).to_csv(\"FILE_PATH\", index=False, encoding=\"utf-8\", lineterminator=\"\\n\")\n",
    "print(\" Pseudo-labeling 後立即預測的結果已儲存：1p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2c008-07ca-41d3-9e14-453676bc42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb_s = model_lgb.get_params().copy()\n",
    "params_lgb_s.pop('early_stopping_rounds', None)\n",
    "model_lgb_s = lgb.LGBMClassifier(**params_lgb_s)\n",
    "\n",
    "params_xgb_s = model_xgb.get_params().copy()\n",
    "params_xgb_s.pop('early_stopping_rounds', None)\n",
    "model_xgb_s = xgb.XGBClassifier(**params_xgb_s)\n",
    "\n",
    "params_cat = model_cat.get_params().copy()         \n",
    "params_cat.pop('early_stopping_rounds', None)\n",
    "params_cat.pop('task_type', None)\n",
    "model_cat_s = CatBoostClassifier(**params_cat, task_type=\"CPU\")\n",
    "\n",
    "base_estimators = [\n",
    "    ('lgb', model_lgb_s),\n",
    "    ('xgb', model_xgb_s),\n",
    "    ('cat', model_cat_s),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=300, max_depth=6, random_state=42, n_jobs=-1))]\n",
    "\n",
    "meta_features = [\n",
    "    \"val_lgb\", \"val_xgb\", \"val_cat\",\n",
    "    \"min_prob\", \"max_prob\", \"mean_prob\", \"std_prob\",\n",
    "    \"rank_lgb\", \"rank_xgb\", \"rank_cat\",\n",
    "    \"mean_rank\", \"std_rank\", \"softmax_mean\",\n",
    "    \"top1_agree\", \"rank_ptp\", \"rank_bias\"]\n",
    "\n",
    "meta_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegressionCV(Cs=10, cv=5, scoring='f1', max_iter=1000, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    passthrough=True, n_jobs=-1)\n",
    "\n",
    "meta_model.fit(meta_X, y_aug)\n",
    "oof_avg = meta_model.predict_proba(pd.DataFrame(meta_X, columns=meta_features))[:, 1]\n",
    "final_probs = meta_model.predict_proba(pd.DataFrame(meta_test, columns=meta_features))[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.0, 0.99, 0.01)\n",
    "best_f1, best_thresh = 0, 0.5\n",
    "for t in thresholds:\n",
    "    f1 = f1_score(y_aug, (oof_avg > t).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thresh = f1, t\n",
    "        \n",
    "auc = roc_auc_score(y_aug, oof_avg)\n",
    "print(f\"AUC = {auc:.4f}\")\n",
    "final_preds = (final_probs > best_thresh).astype(int)\n",
    "\n",
    "print(f\"F1 Score = {f1_score(y_aug, (oof_avg > best_thresh).astype(int)):.4f} \"\n",
    "      f\"Precision = {precision_score(y_aug, (oof_avg > best_thresh).astype(int)):.4f} \"\n",
    "      f\"Recall = {recall_score(y_aug, (oof_avg > best_thresh).astype(int)):.4f} \"\n",
    "      f\"Threshold = {best_thresh:.4f}\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    meta_model, meta_X, y_aug,\n",
    "    scoring='f1', n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": meta_features,\n",
    "    \"Mean_Importance\": result.importances_mean,\n",
    "    \"Std_Dev\": result.importances_std\n",
    "}).sort_values(by=\"Mean_Importance\", ascending=False)\n",
    "\n",
    "print(\" Permutation Importance (Top 5):\")\n",
    "print(importance_df.head())\n",
    "\n",
    "selected_meta_features = importance_df[importance_df[\"Mean_Importance\"] > 0][\"Feature\"].tolist()\n",
    "if len(selected_meta_features) == 0:\n",
    "    print(\" 無重要特徵被選中，將 fallback 使用全部 meta features\")\n",
    "    selected_meta_features = meta_features\n",
    "print(\" 選中的 meta features:\", selected_meta_features)\n",
    "\n",
    "meta_X_selected = pd.DataFrame(meta_X, columns=meta_features)[selected_meta_features].to_numpy()\n",
    "meta_test_selected = pd.DataFrame(meta_test, columns=meta_features)[selected_meta_features].to_numpy()\n",
    "meta_model_new = LogisticRegressionCV(Cs=10, cv=5, scoring='f1', max_iter=1000,\n",
    "                                      class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "meta_model_new.fit(meta_X_selected, y_aug)\n",
    "\n",
    "oof_selected = meta_model_new.predict_proba(pd.DataFrame(meta_X_selected, columns=selected_meta_features))[:, 1]\n",
    "final_probs_retrained = meta_model_new.predict_proba(pd.DataFrame(meta_test_selected, columns=selected_meta_features))[:, 1]\n",
    "\n",
    "best_f1, best_thresh = 0, 0.5\n",
    "for t in thresholds:\n",
    "    f1 = f1_score(y_aug, (oof_selected > t).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thresh = f1, t\n",
    "        \n",
    "auc = roc_auc_score(y_aug, oof_selected)\n",
    "print(f\"AUC = {auc:.4f}\")\n",
    "print(f\" New F1 after feature selection: {best_f1:.4f}\")\n",
    "final_preds = (final_probs_retrained > best_thresh).astype(int)\n",
    "test_probs = final_probs_retrained[is_test]\n",
    "top_test_idx = np.argsort(-test_probs)[:fixed_test_topk]\n",
    "final_preds[is_test] = 0\n",
    "final_preds[np.where(is_test)[0][top_test_idx]] = 1\n",
    "\n",
    "private_probs = final_probs_retrained[is_private]\n",
    "n_top_private = max(1, int(len(private_probs) * top_pct))\n",
    "top_private_idx = np.argsort(-private_probs)[:n_top_private]\n",
    "final_preds[is_private] = 0\n",
    "final_preds[np.where(is_private)[0][top_private_idx]] = 1\n",
    "\n",
    "print(f\"選中：test={final_preds[is_test].sum()}，private={final_preds[is_private].sum()}\")\n",
    "\n",
    "pd.DataFrame({\"ID\": ids, \"飆股\": final_preds, \"dataset\": dataset_flag})[[\"ID\", \"飆股\"]].to_csv(\"FILE_PATH\", index=False, encoding=\"utf-8\", lineterminator=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIGO)",
   "language": "python",
   "name": "aigo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
